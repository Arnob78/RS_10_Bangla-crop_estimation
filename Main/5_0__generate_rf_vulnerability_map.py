import os
import sys
import numpy as np
import pandas as pd
import joblib
import rasterio
from tqdm import tqdm

def main():
    # --- Configuration ---
    # Auto-detect script name for folder creation
    script_path = os.path.abspath(__file__)
    script_filename = os.path.basename(script_path)
    SCRIPT_NAME, _ = os.path.splitext(script_filename)

    print(f"Script name detected: {SCRIPT_NAME}")
    
    # Use the same PROJECT_ROOT as in your training script
    PROJECT_ROOT = "/srv/AB_YU/RS_10_Bangla-crop_estimation"
    
    # Define paths based on earlier scripts
    DATA_DIR = os.path.join(PROJECT_ROOT, "2.5__preprocess_for_training")
    
    # Use the RF model from the corrected training script
    # The training script creates folder named "7__train_random_forest-correct"
    MODEL_DIR = os.path.join(PROJECT_ROOT, "7__train_random_forest-correct")
    
    MASK_DIR = os.path.join(PROJECT_ROOT, "1__create_cropland_mask")
    
    MODEL_PATH = os.path.join(MODEL_DIR, "random_forest_model.joblib")
    CROP_MASK_PATH = os.path.join(MASK_DIR, "cropland_mask.tif")

    # Auto-create output directory with script name
    OUTPUT_DIR = os.path.join(PROJECT_ROOT, SCRIPT_NAME)
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    OUTPUT_RASTER = os.path.join(OUTPUT_DIR, "rf_crop_vulnerability_map.tif")
    
    # Additional output files
    OUTPUT_STATS = os.path.join(OUTPUT_DIR, "vulnerability_statistics.csv")
    OUTPUT_REPORT = os.path.join(OUTPUT_DIR, "vulnerability_generation_report.txt")

    print(f"--- Generating Vulnerability Map with Random Forest Model ---")
    print(f"Project root: {PROJECT_ROOT}")
    print(f"Data directory: {DATA_DIR}")
    print(f"Model directory: {MODEL_DIR}")
    print(f"Using model: {MODEL_PATH}")
    print(f"Output will be saved in: {OUTPUT_DIR}")
    
    # 1. Load Model
    print("\n[Step 1] Loading Random Forest model...")
    try:
        model = joblib.load(MODEL_PATH)
        print(f"✓ Model loaded successfully")
        print(f"  Model type: {type(model)}")
        if hasattr(model, 'n_estimators'):
            print(f"  Number of trees: {model.n_estimators}")
        if hasattr(model, 'n_features_in_'):
            print(f"  Expected features: {model.n_features_in_}")
    except FileNotFoundError:
        print(f"✗ FATAL ERROR: Model not found at {MODEL_PATH}")
        print(f"  Make sure you have run the training script first: 7__train_random_forest-correct.py")
        return
    except Exception as e:
        print(f"✗ ERROR loading model: {e}")
        return
        
    # 2. Load metadata and full dataset
    print("\n[Step 2] Loading metadata and full dataset for prediction...")
    try:
        metadata_path = os.path.join(DATA_DIR, "metadata.npy")
        metadata = np.load(metadata_path, allow_pickle=True).item()
        
        temporal_shape = metadata['temporal_shape']
        static_shape = metadata['static_shape']
        target_shape = metadata['target_shape']
        
        print(f"✓ Metadata loaded")
        print(f"  Temporal shape: {temporal_shape}")
        print(f"  Static shape: {static_shape}")
        print(f"  Target shape: {target_shape}")
        
        # Calculate total features
        num_pixels, num_years, num_temporal_features = temporal_shape
        num_static_features = static_shape[1]
        total_features = num_years * num_temporal_features + num_static_features
        print(f"  Total features after flattening: {total_features}")
        
        # Load memory-mapped files
        temporal_mmap_path = os.path.join(DATA_DIR, "temporal_features.mmap")
        static_mmap_path = os.path.join(DATA_DIR, "static_features.mmap")
        
        print(f"Loading temporal features from: {temporal_mmap_path}")
        temporal_features = np.memmap(temporal_mmap_path, dtype='float32', mode='r', shape=temporal_shape)
        
        print(f"Loading static features from: {static_mmap_path}")
        static_features = np.memmap(static_mmap_path, dtype='float32', mode='r', shape=static_shape)
        
        # Load valid cropland coordinates
        valid_cropland_y_path = os.path.join(DATA_DIR, "cropland_y_valid.npy")
        valid_cropland_x_path = os.path.join(DATA_DIR, "cropland_x_valid.npy")
        
        valid_cropland_y = np.load(valid_cropland_y_path)
        valid_cropland_x = np.load(valid_cropland_x_path)
        
        print(f"✓ Loaded valid cropland coordinates")
        print(f"  Number of valid cropland pixels: {len(valid_cropland_y)}")
        
        # Load mask for georeferencing
        with rasterio.open(CROP_MASK_PATH) as src:
            height, width = src.height, src.width
            transform = src.transform
            crs = src.crs
            
        print(f"✓ Loaded mask for georeferencing")
        print(f"  Mask dimensions: {height} x {width}")
        print(f"  CRS: {crs}")

    except FileNotFoundError as e:
        print(f"✗ FATAL ERROR: File not found: {e}")
        print("This script requires the outputs from '2.5__preprocess_for_training.py'.")
        print(f"Make sure all files exist in: {DATA_DIR}")
        return
    except Exception as e:
        print(f"✗ ERROR during data loading: {e}")
        import traceback
        traceback.print_exc()
        return
        
    # 3. Prepare data for Random Forest (flatten and combine)
    print("\n[Step 3] Preparing full dataset for prediction...")
    
    print("Flattening temporal data...")
    X_temporal_flat = temporal_features.reshape(num_pixels, -1)
    
    print("Combining temporal and static features...")
    X_full = np.concatenate([X_temporal_flat, static_features], axis=1)
    
    print(f"✓ Full feature matrix created with shape: {X_full.shape}")
    
    # Memory usage estimation
    memory_estimate = X_full.nbytes / (1024**3)  # GB
    print(f"  Estimated memory usage for feature matrix: {memory_estimate:.2f} GB")
    
    # 4. Make Predictions
    print("\n[Step 4] Making predictions on the full dataset...")
    print("WARNING: This may be memory intensive and take some time.")
    
    try:
        # Use tqdm for progress bar if available
        try:
            from tqdm import tqdm
            
            # Predict in batches if needed, otherwise predict all at once
            print("Starting predictions...")
            full_predictions = model.predict(X_full)
            
            print(f"✓ Predictions complete")
            print(f"  Number of predictions: {len(full_predictions)}")
            print(f"  Prediction shape: {full_predictions.shape}")
            
        except ImportError:
            print("tqdm not available, predicting without progress bar...")
            full_predictions = model.predict(X_full)
            print(f"✓ Predictions complete")
            
    except MemoryError:
        print("\n✗ FATAL ERROR: MemoryError occurred during prediction.")
        print("The machine does not have enough RAM to hold the feature matrix and perform predictions.")
        print("Consider predicting in batches or using a machine with more memory.")
        return
    except Exception as e:
        print(f"✗ ERROR during prediction: {e}")
        import traceback
        traceback.print_exc()
        return
        
    # 5. Reconstruct the 2D map
    print("\n[Step 5] Reconstructing 2D vulnerability map...")
    
    # Create empty raster with nodata value
    vulnerability_map = np.full((height, width), -9999.0, dtype=np.float32)
    
    # Fill in predictions at valid cropland locations
    print(f"Filling {len(valid_cropland_y)} valid cropland pixels...")
    vulnerability_map[valid_cropland_y, valid_cropland_x] = full_predictions.astype(np.float32)
    
    print(f"✓ 2D map reconstructed")
    
    # 6. Analyze and save the final map
    print("\n[Step 6] Analyzing and saving the final map...")
    
    # Calculate statistics
    valid_pixels = vulnerability_map[vulnerability_map != -9999.0]
    valid_count = len(valid_pixels)
    nodata_count = height * width - valid_count
    
    mean_vulnerability = np.mean(valid_pixels)
    std_vulnerability = np.std(valid_pixels)
    min_vulnerability = np.min(valid_pixels)
    max_vulnerability = np.max(valid_pixels)
    
    print(f"\n--- Vulnerability Map Analysis ---")
    print(f"Total pixels in raster: {height * width:,}")
    print(f"Valid (cropland) pixels: {valid_count:,}")
    print(f"NoData pixels: {nodata_count:,}")
    print(f"Mean Vulnerability: {mean_vulnerability:.4f}")
    print(f"Standard Deviation: {std_vulnerability:.4f}")
    print(f"Min Vulnerability: {min_vulnerability:.4f}")
    print(f"Max Vulnerability: {max_vulnerability:.4f}")
    
    # Calculate percentiles for better understanding
    percentiles = [5, 25, 50, 75, 95]
    percentile_values = np.percentile(valid_pixels, percentiles)
    
    print(f"\nVulnerability Percentiles:")
    for p, val in zip(percentiles, percentile_values):
        print(f"  {p}th percentile: {val:.4f}")
    
    # Save statistics to CSV
    stats_data = {
        'Statistic': ['Total Pixels', 'Valid Pixels', 'NoData Pixels', 
                      'Mean', 'Std Dev', 'Min', 'Max',
                      '5th Percentile', '25th Percentile', 'Median (50th)', 
                      '75th Percentile', '95th Percentile'],
        'Value': [height * width, valid_count, nodata_count,
                  mean_vulnerability, std_vulnerability, 
                  min_vulnerability, max_vulnerability,
                  percentile_values[0], percentile_values[1], percentile_values[2],
                  percentile_values[3], percentile_values[4]]
    }
    
    stats_df = pd.DataFrame(stats_data)
    stats_df.to_csv(OUTPUT_STATS, index=False)
    print(f"✓ Statistics saved to: {OUTPUT_STATS}")
    
    # Save raster file
    meta = {
        'driver': 'GTiff',
        'height': height,
        'width': width,
        'count': 1,
        'dtype': 'float32',
        'crs': crs,
        'transform': transform,
        'nodata': -9999.0,
        'compress': 'lzw'
    }
    
    with rasterio.open(OUTPUT_RASTER, 'w', **meta) as dst:
        dst.write(vulnerability_map, 1)
        dst.update_tags(
            description='Crop Vulnerability Map generated by Random Forest',
            model_source=os.path.basename(MODEL_PATH),
            generation_date=pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),
            units='Vulnerability Index',
            min_value=float(min_vulnerability),
            max_value=float(max_vulnerability),
            mean_value=float(mean_vulnerability)
        )
        
    print(f"✓ Vulnerability map saved to: {OUTPUT_RASTER}")
    
    # Create a summary report
    with open(OUTPUT_REPORT, 'w') as f:
        f.write("="*60 + "\n")
        f.write("CROP VULNERABILITY MAP GENERATION REPORT\n")
        f.write("="*60 + "\n\n")
        
        f.write("1. INPUT INFORMATION\n")
        f.write("="*40 + "\n")
        f.write(f"Model used: {MODEL_PATH}\n")
        f.write(f"Data directory: {DATA_DIR}\n")
        f.write(f"Mask file: {CROP_MASK_PATH}\n")
        f.write(f"Generation date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write("2. DATA STATISTICS\n")
        f.write("="*40 + "\n")
        f.write(f"Total pixels in raster: {height * width:,}\n")
        f.write(f"Valid (cropland) pixels: {valid_count:,}\n")
        f.write(f"NoData pixels: {nodata_count:,}\n")
        f.write(f"Model features used: {total_features}\n\n")
        
        f.write("3. VULNERABILITY STATISTICS\n")
        f.write("="*40 + "\n")
        f.write(f"Mean Vulnerability: {mean_vulnerability:.4f}\n")
        f.write(f"Standard Deviation: {std_vulnerability:.4f}\n")
        f.write(f"Minimum Vulnerability: {min_vulnerability:.4f}\n")
        f.write(f"Maximum Vulnerability: {max_vulnerability:.4f}\n\n")
        
        f.write("4. VULNERABILITY PERCENTILES\n")
        f.write("="*40 + "\n")
        for p, val in zip(percentiles, percentile_values):
            f.write(f"{p}th percentile: {val:.4f}\n")
        
        f.write("\n5. OUTPUT FILES\n")
        f.write("="*40 + "\n")
        f.write(f"Vulnerability map: {OUTPUT_RASTER}\n")
        f.write(f"Statistics CSV: {OUTPUT_STATS}\n")
        f.write(f"This report: {OUTPUT_REPORT}\n")
        
        f.write(f"\nAll files saved in: {OUTPUT_DIR}\n")
    
    print(f"✓ Report saved to: {OUTPUT_REPORT}")
    
    print(f"\n" + "="*60)
    print("SCRIPT COMPLETED SUCCESSFULLY!")
    print("="*60)
    print(f"\nAll results saved in: {OUTPUT_DIR}")
    print(f"Vulnerability map: {OUTPUT_RASTER}")
    print(f"Statistics: {OUTPUT_STATS}")
    print(f"Report: {OUTPUT_REPORT}")

if __name__ == "__main__":
    main()